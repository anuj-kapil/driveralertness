---
title: "Untitled"
output: word_document
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(caret)
library(mlbench)
library(fscaret)
library(dplyr)
library(psych)
library(corrplot)
library(xgboost)
library(ROCR)
library(pROC)
library(gridExtra)
#library(gbm)
#library(e107)
```

# Preliminary Data Analysis

## Source

The dataset is publically available on kaggle’s website as a part of competition held five years ago on 19th Jan, 2011 under the name of “Stay Alert! The Ford Challenge”.

```{r}
##### Reading the files for analysis #####
# Train Data would be used for training the model and testing the model performance
data <- read.csv("Data/fordTrain.csv",
                 header=TRUE, stringsAsFactors=FALSE, na.strings = c("NA", ""),
                 strip.white = TRUE, blank.lines.skip=TRUE, skip=0)

# Test Data would be used as a holdout set for model validation
validatedata <- read.csv("Data/fordTest.csv",
                         header=TRUE, stringsAsFactors=FALSE, na.strings = c("NA", ""),
                         strip.white = TRUE, blank.lines.skip=TRUE, skip=0)

dim(data)
```

## Data Description

The dataset of 604329 observations consists of 100 drivers of both genders, of different ages and ethnic backgrounds, who have been sampled a total of 500 times against 3 key sets of variables. There are 33 attributes in the dataset in total.  

* Physiological (8 features) defined simply as P1 to P8.
* Environmental (11 features) defined simply as E1 to E11.
* Vehicular (11 features) defined simply as V1 to V11.  

Each driver trail has been recorded in a simulated driving environment for a period of 2 minutes and an observation recorded every 100 milliseconds. Each driver’s trail has been defined uniquely and labelled at TrialID and every observation within each trail is defined uniquely and labelled as ObsNum.  

The objective is to design a classifier that will detect whether the driver is alert or not alert using predictors like the driver’s physiological attributes combined with vehicular and environmental attributes acquired from the simulated environment. The outcome variable is also provide in the dataset and is labelled as IsAlert and is a binary (0 or 1) outcome where ‘1’ means alert and ‘0’ means not alert.  



```{r}
##### Printing the distribution of Result #####
counts <- table(data$IsAlert)
countsframe<-as.data.frame(counts)

ggplot(countsframe, aes(x = Var1, y = Freq)) +
  geom_bar(stat = "identity", fill = '#000099') +
  geom_text(aes(label = sprintf("%.2f%%", Freq/sum(Freq) * 100))  , 
            vjust = -.5)+
  scale_size_area() +
  ggtitle("Alertness Distribution") + xlab("IsAlert") + ylab("Frequency")

```

From the above plot, it is noted that 42.12% of the observations are not alert, which means the dataset is fairly balanced for modelling assuming we don’t drop any observation in data pre-processing.  

## Descriptive statistics and analysis

**Dataset Assumption**: It is assumed that the dataset is collected in same simulated environment using the same sensors for all trials of different drivers. 

**Descriptive statistics**:

```{r}
#Descriptive Statistics
psych::describe(data, fast = FALSE )
```

The primary issue at play is the lack of data dictionary. The various predictor variables are merely categorised in to three types of attributes as Physiological, Environmental and Vehicular and simply labelled as P1-P8, E1-E11 and V1-V11. The descriptive stats show that  

* All the variables/attributes are numerical as categorical variables are usually listed with an asterisk in the descriptive summary. However looking at low range of some of the variables like E3, E8, E9, V5, V10 it can be assumed that the variables may be categorical.
* One particular variable (P6) is highly skewed having a max value of 228812 and a mean if just 845.38 which suggest some extreme outliers. We will have look at outliers in the outliers’ analysis section.
* Variable P8, V7 and V9 are all zeroes irrespective of the response variable being 0 or 1. These three variables will be dropped from the dataset for modelling purpose.
* Looking at the skewness, many of the variables are not ‘normal’ in their distribution. This affects the choice of models that be applied to the dataset.  

The response variable IsAlert is definitely a categorical variable with possible values of 1 (Alert) and 0 (Not Alert). For modelling purpose, we will convert the response variable in to a factor variable.  

**Missing Value Analysis**:

```{r}
#No missing values
summary(data)
```

Looking at the summary of the dataset, we can see that none of the variables have missing values otherwise the variable would have been summarized with a statistics of NA’s having a valid positive integer values.  

**Repeated Measure Analysis**:

```{r}
########### Repeated Measure Analysis ################
hist_data <- data %>% count(TrialID)
hist(hist_data$n, main = "Histogram of repeated measures for each driver", 
     xlab="Number of Repeated Measures by Driver", ylab="Frequency", col = "blue"
     , breaks=seq(1160,1220,by=10))
```

**Outlier Analysis**:
Box plots and histograms of individual attributes can be found in the appendix.  

All variables have some outliers. In some cases these are quite extreme. We are unable to determine if these from data quality issues or true values as we don’t have the data description of each of the attributes.  

Looking at the box plot of P5, P6, E3, E7, E11 and V4 suggests that the data consists mostly of zeroes. However we can’t simply exclude them from analysis as outliers might be a key contributor in predicting the outcome variable. It would be good to look at the distribution of outcome variable for these attributes. For this we will split the dataset based on outcome variable as alert and not alert dataset. Looking at the overlaid histogram (included in the appendix) of the above attributes, we can see that these variables are almost equally distributed among the two subsets. Hence, we will include these attributes in our analysis.  


**Multicollinearity Analysis**:
Physiological attributes 3&4 and Vehicular attributes 1 is highly correlated with vehicular attribute 6 &10.  Only 1 from each mentioned set of correlated attributes will likely be used for this analysis. The plot below shows the collinearity among various attributes. Usually a threshold of .75 for coefficient of correlation is considered to be of high strength relationship. 

```{r}
############ Correlation Analysis #####################
cor_data_full <- data[,!(names(data) %in% c("IsAlert","TrialID","ObsNum","P8","V7","V9"))]

##### Computing the correlation matrix ######
cor_mat_full <- cor(cor_data_full, use="complete.obs")

##### Computing the correlation matrix ######
corrplot.mixed(cor_mat_full, lower="number", upper="circle")
```

#Data Preparation

## Variable Importance

We have already dropped three variables namely P8, V7 and V9 because they are all zeroes. We have already done the collinearity analysis to find out the highly correlated variables, which will help us in dimensionality reduction.  

Another technique which we are going to use is Ensemble Feature Selection using fscaret package in R to find out individual variable importance. The fscaret package is closely related to caret package in R and uses the underlying caret function to get its job done. The ensemble feature selection takes in a data set and a list of models and, in return, fscaret will scale and return the importance of each variable for each model and for the ensemble of models. The tool extracts the importance of each variable by using the selected models’ VarImp or similar measuring function. For example, linear models use the absolute value of the t-statistic for each parameter and decision-tree models, total the importance of the individual trees, etc. It returns individual and combined MSEs and RMSEs:   

MSE (Mean Squared Error): the variance of the estimator  
RMSE (Root Mean Squared Error): the standard deviation of the sample  

For this technique to work, the data needs to be formatted in multiple in, single out (MISO) format. Also the output needs to be the last column in the data frame. Since our outcome variable IsAlert is not the last column, we will need to format the dataset.  

```{r}
#Formatting Dataset
data<-data[c("TrialID", "ObsNum", "P1", "P2", "P3", "P4", "P5", 
             "P6", "P7", "P8", "E1", "E2", "E3", "E4", "E5", "E6", "E7", "E8", 
             "E9", "E10", "E11", "V1", "V2", "V3", "V4", "V5", "V6", "V7", 
             "V8", "V9", "V10", "V11", "IsAlert")]
```

As mention in the descriptive analysis, some of the variables in the dataset appear to be categorical in nature. We will convert those variables to factor using a custom function.  

```{r}
# Convert any potential factors in the data through heuristic.
# If number of unique values in dataset is less than specified threshold
# then treat as categorical data
auto_convert_factors <- function(data, cat_threshold=10, cols_ignore=list()) {
  
  for (col in names(data)) {
    if (!is.factor(data[[col]]) && 
        length(unique(data[[col]])) <= cat_threshold && 
        !is.element(col, cols_ignore)) {
      data[[col]] <- as.factor(data[[col]])
      cat(col, " converted to factor\n")
    }
  }
  data
}

isAlertData <- auto_convert_factors(data, 10, cols_ignore = list('IsAlert'))
```

Next step is to dummify the factor variables. To do that, we will use some of the caret functions.  

```{r}
#Dropping P8, V7, V9 as they are all zeroes. Dropping TrailID and ObsNum as they are unique ids.
isAlertData<-isAlertData[c("P1", "P2", "P3", "P4", "P5", 
                           "P6", "P7", "E1", "E2", "E3", "E4", "E5", "E6", "E7", "E8", 
                           "E9", "E10", "E11", "V1", "V2", "V3", "V4","V5", "V6", "V8",
                           "V10", "V11", "IsAlert")]

datadummy<-dummyVars("~.",data=isAlertData,fullRank = F)
datatemp<-as.data.frame(predict(datadummy,isAlertData))
```

We now need a training and a test dataset, for which, again we will use a **caret** function called *createDataPartition*.  

```{r}
#Partitioning dataset in to training and test dataset.
splitIndexMulti <- createDataPartition(datatemp$IsAlert, p=.01, list = FALSE, times = 2)

trainDataset <- datatemp[splitIndexMulti[,1],]
testDataset <- datatemp[splitIndexMulti[,2],]

dim(datatemp)
dim(trainDataset)
dim(testDataset)
```

Finally, we need to select an ensemble of models and feed the data and list of models to the main function of the fscaret package named as its package, fscaret. Since our problem is a classification problem, we can choose either the models specific for classification or dual purpose modes (classification and regression). We have chosen seven models here.  

```{r}

#Variable Importance
fsModels2 <- c("glm","gbm","treebag","ridge","lasso","rf","xgbLinear")
myFs2<-fscaret(trainDataset, testDataset, myTimeLimit = 40, preprocessData = TRUE,
               Used.funcRegPred = fsModels2, with.labels = TRUE,
               supress.output=FALSE, no.cores = 2, installReqPckg = TRUE)
```

The output of the fscaret function (myFs) holds a lot of information. One of the most interesting result set is the $VarImp$matrixVarImp.MSE (Mean Squared Error). This returns the top variables from the perspective of all models involved (the MSE is scaled to compare each model equally):  

```{r}
names(myFs2)
myFs2$VarImp
myFs2$PPlabels
myFs2$VarImp$matrixVarImp.MSE
```

The input_no is actually the numeric label for each of the attributes. This can be reformatted as shown below to show the actual variables and are listed in the descending order of their importance. For example, the above output, input_no 28 means V11 variable and the input_no 19 is the E9.1 variable which is the dummified version of E9 variable.  

```{r}
results <- myFs2$VarImp$matrixVarImp.MSE
results$Input_no <- as.numeric(results$Input_no)
results <- results[c("SUM","SUM%","ImpGrad","Input_no")]
myFs2$PPlabels$Input_no <-  as.numeric(rownames(myFs2$PPlabels))
results <- merge(x=results, y=myFs2$PPlabels, by="Input_no", all.x=T)
results <- results[c('Labels', 'SUM')]
results <- subset(results,results$SUM !=0)
results <- results[order(-results$SUM),]
print(results)
```

The most import variable is the vehicular attribute 11 to predict the alertness of the driver. The next most important variable is the categorical variable E9 and to be precise the E9 being ‘1’. Also, from the MSE output it is noted that the different models have predicted the variable importance differently. For example, the most important variable as per the GBM, XGBoost models is V11 while E9.1 is the most important variable as per GLM, lasso and ridge. It is also noted that the above function has dropped two variables, namely P4 and V1 from its analysis which we found to be highly correlated with P3 and V6, V10 respectively. So, we can drop the two variables P4 and V1 from our dataset for modelling purposes.  

# Modelling and Evaluation

## Tuning and Modelling

We will again use the caret package in R to build our models and evaluate them. The train function in the package will be used for evaluating the effect of model tuning parameters on performance using resampling, choosing the optimal model across these parameters and estimating the model performance from a training set.  

The first step is choosing a model. We will use Stochastic Gradient Boosting (gbm) and Extreme Gradient Boosting (xgbLinear) for our modelling purpose. Both are dual purpose models and can be used for both classification and regression.  

**Basic Parameter Tuning**  
We will use 5-fold stratified repeated cross validation using the traincontrol function to estimate model performance and generalize the model to limit over fitting.  

```{r}
# Uses caret library, doing Automatic grid search (possible to do manual one as well)

modelTrain<-trainDataset
modelTrain$IsAlert <- as.factor(ifelse(modelTrain$IsAlert == 1,'Y','N'))

modelTest<-testDataset
modelTest$IsAlert <- as.factor(ifelse(modelTest$IsAlert == 1,'Y','N'))

#Defining training control
control <- trainControl(
  method          = "repeatedcv",
  number          = 5,
  repeats         = 2,
  search          = "grid",
  classProbs      = TRUE,
  summaryFunction = twoClassSummary, #ROC AUC 
  verboseIter     = TRUE
)
```

In the above trainControl function, we are asking to compute additional performance metric of the classification model called twoClassSummary. By default, accuracy and Kappa metrics are computed for a classification model. The twoClassSummary function will be used to compute the sensitivity, specificity and area under the ROC curve.  

We will now use the train function of the caret package to train our two models. The same models are trained on the complete dataset and reduced dataset (Removing highly correlated variables). We will customise the tuning process by using pre-processing options of centring, scaling and imputation. We do not have missing values in our dataset, hence imputation won’t be required.  

```{r}
model1 <- train(IsAlert ~ ., 
                data = modelTrain[,!(names(modelTrain) %in% c("TrialID","ObsNum","P8","V7","V9"))], 
                method = "gbm",          
                metric = "ROC",
                na.action = na.pass,
                preProcess = c("center", "scale", "medianImpute"),
                trControl = control)


model2 <- train(IsAlert ~ ., 
                data = modelTrain[,!(names(modelTrain) %in% c("TrialID","ObsNum","P8","V7","V9"))], 
                method = "xgbLinear",          
                metric = "ROC",
                na.action = na.pass,
                preProcess = c("center", "scale", "medianImpute"),
                trControl = control)

# Check out the hyperparameters 
print(model1)
```

This is the model summary for GBM. The train function automatically tunes the hyperparameters based on the largest value of ROC.  
For a gradient boosting machine (gbm) model, the main tuning parameters are:  

* number of iterations, i.e. trees, (called n.trees in the gbm function)
* complexity of the tree, called interaction.depth
* learning rate: how quickly the algorithm adapts, called shrinkage
* the minimum number of training set samples in a node to commence splitting (n.minobsinnode)  

```{r}
# Check out the hyperparameters 
print(model2)
```

This is the model summary for XGBoost. The train function automatically tunes the hyperparameters based on the largest value of ROC.  

For an extreme gradient boosting (xbmLinear) model, the main tuning parameters are:  

* the max number of iterations: nrounds
* L2 regularization term on weights: lambda
* L1 regularization term on weights: alpha
* step size of each boosting step: eta


## Results and Interpretation

**ROC as Evaluation metric**

Area under the ROC curve (AUC) is used for flexibility in deciding between minimizing the false positive rate & maximizing the true positive rate. ROC is also robust against class label imbalance (43:57 for this dataset). It is a commonly used evaluation method for binary outcome problems that involve classifying an instance as either positive or negative. Its main advantages over other evaluation methods, such as the simpler misclassification error, are:  

* It is insensitive to unbalanced datasets.
* For other evaluation methods, a user has to choose a cut-off point above which the target variable is part of the positive class (e.g. a logistic regression model returns any real number between 0 and 1 - the modeller might decide that predictions greater than 0.5 mean a positive class prediction while a prediction of less than 0.5 mean a negative class prediction). AUC evaluates entries at all cut-off points, giving better insight into how well the classifier is able to separate the two classes.  

The key metric used in the interpretation of results is the accuracy computed in the confusion matrix and area under the ROC curves computed using the twoClassSummary function. See appendix for complete ROC curves.  

We have got a separate dataset which we have used to validate our trained models. The validation dataset is completely new dataset for the models as it was never used in training the models. The training and test dataset were derived from the original dataset using createDataPartition function of the caret package and were used to train the models. The first set of results are from the test dataset derived from original dataset and second set of results are the real tests on the validation dataset which consists of 120840 observation having the same number and type of attributes as the original dataset.  

The second set of results show that we have over fitted our models since the accuracy and the AUC was significantly dropped when the model is tested on an entirely new dataset. However, based on the results of validation dataset, we can conclude that the extreme gradient boost model performed better than the gradient boost machine.  

# Recommended Classifier

Gradient Boosting machines are ensemble models with the goal to build a series of under fitted (unlike random forest’s over-fitted) models, each reducing the errors of previous model where cumulative prediction is used to make the final prediction (Mayr et al, 2014).  

A specific, open-source Extreme Gradient Boosting Model that is fast, scalable and produces state-of-the-art results on a wide range of problems (Chen & Guestrin, 2016) is the recommend classifier for the driver alertness problem. XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting(also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment(Hadoop, SGE, MPI) and can solve problems beyond billions of examples.  

The recommendation is based on the key metric of Area under the ROC curve where XGBoost with complete set of variables as predictors stands out as the best model. Although the GBM performed at par with XGBoost if we look at the accuracy, yet the XBBoost’s area under the curve is better than GBM.  

It should be noted that the recommendation made is based on the two models selected for the current experimentation and other classification model were not tried and tested. Also, due to lack high end resources the training dataset was chosen to be very small as choosing a larger training dataset was slowing down the personal laptop and consuming a considerable amount of time.  








